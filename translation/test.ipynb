{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewly/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewly/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_src = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-de-en')  # German tokenizer\n",
    "# tokenizer_trg = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')  # English tokenizer\n",
    "tokenizer_src = AutoTokenizer.from_pretrained('dbmdz/german-gpt2')  # German tokenizer\n",
    "tokenizer_trg = AutoTokenizer.from_pretrained('openai-community/gpt2')  # English tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer_src, src_language, trg_language, max_length):\n",
    "    inputs = [example[src_language] for example in examples[\"translation\"]]\n",
    "    targets = [example[trg_language] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer_src(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer_trg(targets, max_length=max_length, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "def prepare_data(tokenizer_src, tokenizer_trg, batch_size=4, num_workers=2, test_fraction=0.2, max_length=512):\n",
    "    # Load dataset; ignore validation set (tst2013) and use test set only (tst2014)\n",
    "    src_language, trg_language = 'de', 'en'\n",
    "    dataset = load_dataset(\"ted_talks_iwslt\", language_pair=(src_language, trg_language), year=\"2014\")\n",
    "    dataset = dataset.train_test_split(test_size=test_fraction, shuffle=True)\n",
    "    trainset, testset = dataset['train'], dataset['test']\n",
    "    # Preprocess datasets\n",
    "    tokenized_trainset = trainset.map(lambda examples: preprocess_function(examples, tokenizer_src, src_language, tokenizer_trg, trg_language, max_length), batched=True)\n",
    "    tokenized_testset = testset.map(lambda examples: preprocess_function(examples, tokenizer_src, src_language, tokenizer_trg, trg_language, max_length), batched=True)\n",
    "    # Create dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(tokenized_trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = torch.utils.data.DataLoader(tokenized_testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewly/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for ted_talks_iwslt contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ted_talks_iwslt\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|██████████| 1.67G/1.67G [04:25<00:00, 6.27MB/s] \n",
      "Generating train split: 2972 examples [00:06, 461.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ted_talks_iwslt\", language_pair=(\"de\", \"en\"), year=\"2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 298/298 [00:00<00:00, 2442.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_testset = testset.map(lambda examples: preprocess_function(examples, tokenizer_src, \"de\", \"en\", 128), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(tokenized_testset, batch_size=2, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x184fbe3d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "ix = torch.randint(len(testloader), (batch_size,))\n",
    "x = torch.stack([data.dataset[i][0] for i in ix])\n",
    "y = torch.tensor([data.dataset[i][1] for i in ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"beta\": 1,\n",
    "    \"bandwidth\": 1,\n",
    "    \"sphere_radius\": 1,\n",
    "    \"hidden_size\": 1,\n",
    "    \"num_encoder_layers\": 1,\n",
    "    \"num_decoder_layers\": 1,\n",
    "    \"num_attention_heads\": 1,\n",
    "    \"intermediate_size\": 4, # 4 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0,\n",
    "    \"encoder_dropout_prob\": 0,\n",
    "    \"decoder_dropout_prob\": 0,\n",
    "    \"attention_probs_dropout_prob\": 0,\n",
    "    \"initializer_range\": 0.1,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    \"src_vocab_size\": tokenizer_src.vocab_size,\n",
    "    \"src_pad_token_id\": tokenizer_src.pad_token_id,\n",
    "    \"trg_vocab_size\": tokenizer_trg.vocab_size,\n",
    "    \"trg_pad_token_id\": tokenizer_trg.pad_token_id,\n",
    "    \"max_length\": 128,\n",
    "}\n",
    "model = FNSForTranslation(config)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNSForTranslation(\n",
       "  (encoder): FNSEncoder(\n",
       "    (token_embedding): Embedding(58101, 1, padding_idx=58100)\n",
       "    (positional_embedding): Embedding(128, 1)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): FNSEncoderBlock(\n",
       "        (attention): FasterFNSMultiHeadAttention(\n",
       "          (qkv_projection): Linear(in_features=1, out_features=3, bias=True)\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (output_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (output_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (dense_1): Linear(in_features=1, out_features=4, bias=True)\n",
       "          (activation): NewGELUActivation()\n",
       "          (dense_2): Linear(in_features=4, out_features=1, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): FNSDecoder(\n",
       "    (token_embedding): Embedding(58101, 1, padding_idx=58100)\n",
       "    (positional_embedding): Embedding(128, 1)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): FNSDecoderBlock(\n",
       "        (self_attention): FasterFNSMultiHeadAttention(\n",
       "          (qkv_projection): Linear(in_features=1, out_features=3, bias=True)\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (output_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (output_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (cross_attention): FasterFNSMultiHeadAttention(\n",
       "          (kv_projection): Linear(in_features=1, out_features=2, bias=True)\n",
       "          (q_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (output_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (output_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm_2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (dense_1): Linear(in_features=1, out_features=4, bias=True)\n",
       "          (activation): NewGELUActivation()\n",
       "          (dense_2): Linear(in_features=4, out_features=1, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=1, out_features=58101, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, tokenizer_trg, max_len, device):\n",
    "    bos_idx = tokenizer_trg.bos_token_id\n",
    "    eos_idx = tokenizer_trg.eos_token_id\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(bos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build causal mask for target \n",
    "        decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int)\n",
    "        decoder_mask = (decoder_mask == 0).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_trg.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m source \u001b[38;5;241m=\u001b[39m testloader\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m source_mask \u001b[38;5;241m=\u001b[39m testloader\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[205], line 355\u001b[0m, in \u001b[0;36mFNSEncoder.forward\u001b[0;34m(self, x, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# Create the position ids from the input token ids. Any padded tokens remain padded.\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    356\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding(position_ids)\n\u001b[1;32m    357\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "source = torch.tensor(testloader.dataset[\"input_ids\"][0])\n",
    "source_mask = testloader.dataset[\"attention_mask\"][0]\n",
    "model.encoder(source, source_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = trainloader\n",
    "    else:\n",
    "        data = testloader\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    x = torch.tensor([testloader.dataset[\"input_ids\"][i] for i in ix]) # (B, N) \n",
    "    y = torch.tensor([testloader.dataset[\"labels\"][i] for i in ix]) # (B, N) \n",
    "    encoder_mask = torch.tensor([testloader.dataset[\"attention_mask\"][i] for i in ix]) # (B, N) \n",
    "    encoder_mask = (encoder_mask.unsqueeze(-1)@encoder_mask.unsqueeze(1)).view(batch_size, 1, config[\"max_length\"], config[\"max_length\"]) # (B,1,N,N)\n",
    "    decoder_mask = torch.stack([(torch.tensor(testloader.dataset[\"attention_mask\"][i] != 0)).unsqueeze(0).int() & causal_mask(config[\"max_length\"]) for i in ix]) # (B,1,N,N)\n",
    "    return x, y    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
