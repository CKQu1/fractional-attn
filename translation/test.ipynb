{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewly/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewly/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_src = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-de-en')  # German tokenizer\n",
    "# tokenizer_trg = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')  # English tokenizer\n",
    "tokenizer_src = AutoTokenizer.from_pretrained('dbmdz/german-gpt2')  # German tokenizer\n",
    "tokenizer_trg = AutoTokenizer.from_pretrained('openai-community/gpt2')  # English tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer_src, src_language, trg_language, max_length):\n",
    "    inputs = [example[src_language] for example in examples[\"translation\"]]\n",
    "    targets = [example[trg_language] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer_src(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer_trg(targets, max_length=max_length, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "def prepare_data(tokenizer_src, tokenizer_trg, batch_size=4, num_workers=2, test_fraction=0.2, max_length=512):\n",
    "    # Load dataset; ignore validation set (tst2013) and use test set only (tst2014)\n",
    "    src_language, trg_language = 'de', 'en'\n",
    "    dataset = load_dataset(\"ted_talks_iwslt\", language_pair=(src_language, trg_language), year=\"2014\")\n",
    "    dataset = dataset.train_test_split(test_size=test_fraction, shuffle=True)\n",
    "    trainset, testset = dataset['train'], dataset['test']\n",
    "    # Preprocess datasets\n",
    "    tokenized_trainset = trainset.map(lambda examples: preprocess_function(examples, tokenizer_src, src_language, tokenizer_trg, trg_language, max_length), batched=True)\n",
    "    tokenized_testset = testset.map(lambda examples: preprocess_function(examples, tokenizer_src, src_language, tokenizer_trg, trg_language, max_length), batched=True)\n",
    "    # Create dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(tokenized_trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = torch.utils.data.DataLoader(tokenized_testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewly/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for ted_talks_iwslt contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ted_talks_iwslt\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|██████████| 1.67G/1.67G [04:25<00:00, 6.27MB/s] \n",
      "Generating train split: 2972 examples [00:06, 461.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ted_talks_iwslt\", language_pair=(\"de\", \"en\"), year=\"2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 298/298 [00:00<00:00, 2442.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_testset = testset.map(lambda examples: preprocess_function(examples, tokenizer_src, \"de\", \"en\", 128), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(tokenized_testset, batch_size=2, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x184fbe3d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "ix = torch.randint(len(testloader), (batch_size,))\n",
    "x = torch.stack([data.dataset[i][0] for i in ix])\n",
    "y = torch.tensor([data.dataset[i][1] for i in ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# https://github.com/tintn/vision-transformer-from-scratch/blob/main/vit.py\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class FNSAttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the FNSMultiHeadAttention module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, beta, bandwidth, sphere_radius, hidden_size, attention_head_size, dropout, bias=True, is_cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        self.beta, self.bandwidth = beta, bandwidth\n",
    "        self.sphere_radius = sphere_radius\n",
    "        \n",
    "        self.is_cross_attention = is_cross_attention\n",
    "\n",
    "    def forward(self, x, encoder_output_states=None):\n",
    "        if encoder_output_states is not None:\n",
    "            assert self.is_cross_attention, \"Please make sure to instantiate class with `Attention(..., is_cross_attention=True)`.\"\n",
    "            query = F.normalize(self.query(x), p=2, dim=-1)\n",
    "            key = F.normalize(self.key(encoder_output_states), p=2, dim=-1)\n",
    "            value = F.normalize(self.value(encoder_output_states), p=2, dim=-1)   \n",
    "        else:\n",
    "            query = F.normalize(self.query(x), p=2, dim=-1)\n",
    "            key = F.normalize(self.key(x), p=2, dim=-1)\n",
    "            value = F.normalize(self.value(x), p=2, dim=-1)                \n",
    "        # print(f'query shape: {query.shape}')\n",
    "        # print(f'key shape: {key.shape}')\n",
    "        # print(f'value shape: {value.shape}')\n",
    "\n",
    "        beta, bandwidth = self.beta, self.bandwidth\n",
    "        sphere_radius = self.sphere_radius\n",
    "        d_intrinsic = self.attention_head_size\n",
    "\n",
    "        # geodesic distance on sphere\n",
    "        eps = 1e-7  # for limiting the divergence from acos\n",
    "        g_dist = torch.acos(torch.clamp(query @ key.transpose(-2, -1), -1+eps, 1-eps)) * sphere_radius\n",
    "        \n",
    "        # Calculate the attention scores\n",
    "        if beta < 2:\n",
    "            attn_score = (1 + g_dist/bandwidth**0.5)**(-d_intrinsic-beta)\n",
    "        else:\n",
    "            attn_score = torch.exp((-g_dist/bandwidth**0.5)**(beta/(beta-1)))\n",
    "        attn_score_shape = attn_score.shape\n",
    "        D_inv = torch.diag_embed(attn_score.sum(-1)**(-1))  # inverse of degree matrix of attn_score\n",
    "        K_tilde = D_inv @ attn_score @ D_inv\n",
    "        attention_probs = F.normalize(K_tilde,p=1,dim=3)  # can do this as the attn weights are always positive\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        # Calculate the attention output\n",
    "        attention_output = attention_probs @ value\n",
    "\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FNSMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, is_cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        # Whether it is cross attention\n",
    "        self.is_cross_attention = is_cross_attention\n",
    "\n",
    "        self.beta = config['beta']\n",
    "        self.bandwidth = config['bandwidth']\n",
    "        self.sphere_radius = config['sphere_radius']     \n",
    "\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = FNSAttentionHead(\n",
    "                self.beta,\n",
    "                self.bandwidth,\n",
    "                self.sphere_radius,\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias,\n",
    "                self.is_cross_attention\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False, encoder_output_states=None):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x, encoder_output_states) for head in self.heads]\n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FasterFNSMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module with some optimizations.\n",
    "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, is_cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.is_cross_attention = is_cross_attention\n",
    "        \n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a linear layer to project the query, key, and value\n",
    "        if self.is_cross_attention:\n",
    "            self.kv_projection = nn.Linear(self.hidden_size, self.all_head_size * 2, bias=self.qkv_bias)\n",
    "            self.q_projection = nn.Linear(self.hidden_size, self.all_head_size, bias=self.qkv_bias)\n",
    "        else:\n",
    "            self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "        self.beta = config['beta']\n",
    "        self.bandwidth = config['bandwidth']\n",
    "        self.sphere_radius = config['sphere_radius']\n",
    "\n",
    "    def forward(self, x, attention_mask=None, output_attentions=False, encoder_hidden_states=None):\n",
    "        # Project the query, key, and value\n",
    "        if encoder_hidden_states is not None:\n",
    "            assert hasattr(\n",
    "                self, \"q_projection\"\n",
    "            ), \"If class is used as cross attention, the weights `q_projection` have to be defined. Please make sure to instantiate class with `Attention(..., is_cross_attention=True)`.\"\n",
    "            query = self.q_projection(x)\n",
    "            kv = self.kv_projection(encoder_hidden_states)\n",
    "            key, value = torch.chunk(kv, 2, dim=-1)\n",
    "        else:\n",
    "            # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
    "            qkv = self.qkv_projection(x)\n",
    "            # Split the projected query, key, and value into query, key, and value\n",
    "            # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
    "            query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        batch_size, sequence_length, _ = query.size()\n",
    "        num_attention_heads, attention_head_size = self.num_attention_heads, self.attention_head_size\n",
    "\n",
    "        beta, bandwidth = self.beta, self.bandwidth\n",
    "        sphere_radius = self.sphere_radius\n",
    "        d_intrinsic = attention_head_size\n",
    "\n",
    "        query = F.normalize(query.view(batch_size, sequence_length, num_attention_heads, attention_head_size).transpose(1, 2), p=2, dim=-1)\n",
    "        key = F.normalize(key.view(batch_size, sequence_length, num_attention_heads, attention_head_size).transpose(1, 2), p=2, dim=-1)\n",
    "        value = value.view(batch_size, sequence_length, num_attention_heads, attention_head_size).transpose(1, 2)\n",
    "        # print(f'query shape: {query.shape}')\n",
    "        # print(f'key shape: {key.shape}')\n",
    "        # print(f'value shape: {value.shape}')        \n",
    "\n",
    "        # geodesic distance on sphere\n",
    "        eps = 1e-7  # for limiting the divergence from acos\n",
    "        g_dist = torch.acos(torch.clamp(query @ key.transpose(-2, -1), -1+eps, 1-eps)) * sphere_radius\n",
    "        \n",
    "        # Calculate the attention scores\n",
    "        if beta < 2:\n",
    "            attn_score = (1 + g_dist/bandwidth**0.5)**(-d_intrinsic-beta)\n",
    "        else:\n",
    "            attn_score = torch.exp((-g_dist/bandwidth**0.5)**(beta/(beta-1)))\n",
    "        D_inv = torch.diag_embed(attn_score.sum(-1)**(-1))  # inverse of degree matrix of attn_score\n",
    "        K_tilde = D_inv @ attn_score @ D_inv\n",
    "        K_tilde = K_tilde.masked_fill(attention_mask.expand(-1,self.num_attention_heads,-1,-1)==0, -1e9) # Mask\n",
    "        attention_probs = F.normalize(K_tilde,p=1,dim=3)  # can do this as the attn weights are always positive\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        # Calculate the attention output\n",
    "        attention_output = attention_probs @ value\n",
    "        # Resize the attention output\n",
    "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        # To (batch_size, sequence_length, all_head_size)\n",
    "        attention_output = attention_output.transpose(1, 2) \\\n",
    "                                           .contiguous() \\\n",
    "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
    "        # Project the attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNSEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.attention = FasterFNSMultiHeadAttention(config)\n",
    "        else:\n",
    "            self.attention = FNSMultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, attention_mask=None, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = \\\n",
    "            self.attention(x, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = self.layernorm_1(x + attention_output)\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(x)\n",
    "        # Skip connection\n",
    "        x = self.layernorm_2(x + mlp_output)\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "        \n",
    "\n",
    "class FNSDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.self_attention = FasterFNSMultiHeadAttention(config)\n",
    "            self.cross_attention = FasterFNSMultiHeadAttention(config, is_cross_attention=True)\n",
    "        else:\n",
    "            self.self_attention = FNSMultiHeadAttention(config)\n",
    "            self.cross_attention = FNSMultiHeadAttention(config, is_cross_attention=True)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_3 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, encoder_output_states, src_mask=None, trg_mask=None, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, self_attention_probs = \\\n",
    "            self.self_attention(x, attention_mask=src_mask, output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = self.layernorm_1(x + attention_output)\n",
    "        # Cross-attention\n",
    "        attention_output, cross_attention_probs = \\\n",
    "            self.cross_attention(x, attention_mask=trg_mask, output_attentions=output_attentions, encoder_output_states=encoder_output_states)\n",
    "        # Skip connection\n",
    "        x = self.layernorm_2(x + attention_output)\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(x)\n",
    "        # Skip connection\n",
    "        x = self.layernorm_3(x + mlp_output)\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, self_attention_probs, cross_attention_probs)\n",
    "\n",
    "\n",
    "class FNSEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.padding_idx = config[\"src_pad_token_id\"]\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"src_vocab_size\"],\n",
    "            embedding_dim=config[\"hidden_size\"],\n",
    "            padding_idx=config[\"src_pad_token_id\"],\n",
    "        )\n",
    "        self.positional_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"max_length\"],\n",
    "            embedding_dim=config[\"hidden_size\"],\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=config[\"encoder_dropout_prob\"])\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_encoder_layers\"]):\n",
    "            block = FNSEncoderBlock(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, attention_mask=None, output_attentions=False):\n",
    "        # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
    "        position_ids = torch.arange(0, x.shape[-1]).to(x.device)\n",
    "        position_embeddings = self.positional_embedding(position_ids)\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "        # Dropout \n",
    "        x = self.dropout(position_embeddings + token_embeddings)\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)    \n",
    "    \n",
    "class FNSDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer decoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, bias=True):\n",
    "        super().__init__()\n",
    "        self.padding_idx = config[\"trg_pad_token_id\"]\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"trg_vocab_size\"],\n",
    "            embedding_dim=config[\"hidden_size\"],\n",
    "            padding_idx=config[\"trg_pad_token_id\"],\n",
    "        )\n",
    "        self.positional_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"max_length\"],\n",
    "            embedding_dim=config[\"hidden_size\"],\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=config[\"decoder_dropout_prob\"])\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_decoder_layers\"]):\n",
    "            block = FNSDecoderBlock(config)\n",
    "            self.blocks.append(block)\n",
    "        # Tie output linear weights to input embedding matrix\n",
    "        self.fc = nn.Linear(config[\"hidden_size\"], config[\"trg_vocab_size\"], bias=bias)\n",
    "        self.fc.weight = self.token_embedding.weight \n",
    "        \n",
    "    def forward(self, x, embedding_output_states, src_mask=None, trg_mask=None, output_attentions=False):\n",
    "        # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
    "        position_ids = torch.arange(0, x.shape[-1]).to(x.device)\n",
    "        position_embeddings = self.positional_embedding(position_ids)\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "        # Dropout \n",
    "        x = self.dropout(position_embeddings + token_embeddings)\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_self_attentions = []\n",
    "        all_cross_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, self_attention_probs, cross_attention_probs = block(x, embedding_output_states, src_mask=src_mask, trg_mask=trg_mask, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_self_attentions.append(self_attention_probs)\n",
    "                all_cross_attentions.append(cross_attention_probs)\n",
    "        # Linear layer\n",
    "        x = self.fc(x)\n",
    "        # Softmax\n",
    "        x = nn.Softmax(dim=-1)(x)\n",
    "        # Return logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_self_attentions, all_cross_attentions)\n",
    "\n",
    "class FNSForTranslation(nn.Module):\n",
    "    \"\"\"\n",
    "    The seq2seq model for neural machine translation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = FNSEncoder(config)\n",
    "        # Create the transformer decoder module\n",
    "        self.decoder = FNSDecoder(config)\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, encoder_mask, decoder_mask, output_attentions=False):\n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, encoder_self_attentions = self.encoder(x, attention_mask = encoder_mask, output_attentions=output_attentions)\n",
    "        # Calculate the decoder's output\n",
    "        decoder_output, decoder_self_attentions, decoder_cross_attentions = self.decoder(x, encoder_output, src_mask=encoder_mask, trg_mask=decoder_mask, output_attentions=output_attentions)\n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (decoder_output, None, None, None)\n",
    "        else:\n",
    "            return (decoder_output, encoder_self_attentions, decoder_self_attentions, decoder_cross_attentions)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"beta\": 1,\n",
    "    \"bandwidth\": 1,\n",
    "    \"sphere_radius\": 1,\n",
    "    \"hidden_size\": 1,\n",
    "    \"num_encoder_layers\": 1,\n",
    "    \"num_decoder_layers\": 1,\n",
    "    \"num_attention_heads\": 1,\n",
    "    \"intermediate_size\": 4, # 4 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0,\n",
    "    \"encoder_dropout_prob\": 0,\n",
    "    \"decoder_dropout_prob\": 0,\n",
    "    \"attention_probs_dropout_prob\": 0,\n",
    "    \"initializer_range\": 0.1,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    \"src_vocab_size\": tokenizer_src.vocab_size,\n",
    "    \"src_pad_token_id\": tokenizer_src.pad_token_id,\n",
    "    \"trg_vocab_size\": tokenizer_trg.vocab_size,\n",
    "    \"trg_pad_token_id\": tokenizer_trg.pad_token_id,\n",
    "    \"max_length\": 128,\n",
    "}\n",
    "model = FNSForTranslation(config)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNSForTranslation(\n",
       "  (encoder): FNSEncoder(\n",
       "    (token_embedding): Embedding(58101, 1, padding_idx=58100)\n",
       "    (positional_embedding): Embedding(128, 1)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): FNSEncoderBlock(\n",
       "        (attention): FasterFNSMultiHeadAttention(\n",
       "          (qkv_projection): Linear(in_features=1, out_features=3, bias=True)\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (output_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (output_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (dense_1): Linear(in_features=1, out_features=4, bias=True)\n",
       "          (activation): NewGELUActivation()\n",
       "          (dense_2): Linear(in_features=4, out_features=1, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): FNSDecoder(\n",
       "    (token_embedding): Embedding(58101, 1, padding_idx=58100)\n",
       "    (positional_embedding): Embedding(128, 1)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): FNSDecoderBlock(\n",
       "        (self_attention): FasterFNSMultiHeadAttention(\n",
       "          (qkv_projection): Linear(in_features=1, out_features=3, bias=True)\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (output_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (output_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (cross_attention): FasterFNSMultiHeadAttention(\n",
       "          (kv_projection): Linear(in_features=1, out_features=2, bias=True)\n",
       "          (q_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (output_projection): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (output_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm_2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (dense_1): Linear(in_features=1, out_features=4, bias=True)\n",
       "          (activation): NewGELUActivation()\n",
       "          (dense_2): Linear(in_features=4, out_features=1, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (layernorm_3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=1, out_features=58101, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, tokenizer_trg, max_len, device):\n",
    "    bos_idx = tokenizer_trg.bos_token_id\n",
    "    eos_idx = tokenizer_trg.eos_token_id\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(bos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build causal mask for target \n",
    "        decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int)\n",
    "        decoder_mask = (decoder_mask == 0).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_trg.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20011,   589,  6737, 11962,    16,   103,   663,    78,  1052, 34217,\n",
       "            45,     2,   541, 26079,   518,   745,     0, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
       "        [12283,   155,  9817,   216,    81, 33602,  5913,  7305,  2337,     0,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
       "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[231], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model.encoder.token_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[222], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[220], line 357\u001b[0m, in \u001b[0;36mFNSEncoder.forward\u001b[0;34m(self, x, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    355\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    356\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding(position_ids)\n\u001b[0;32m--> 357\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Dropout \u001b[39;00m\n\u001b[1;32m    359\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(position_embeddings \u001b[38;5;241m+\u001b[39m token_embeddings)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/frac-attn/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model.encoder(x, encoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "def get_batch():\n",
    "    data = testloader\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    x = torch.tensor([testloader.dataset[\"input_ids\"][i] for i in ix]) # (B, N) \n",
    "    y = torch.tensor([testloader.dataset[\"labels\"][i] for i in ix]) # (B, N) \n",
    "    encoder_mask = torch.tensor([testloader.dataset[\"attention_mask\"][i] for i in ix]) # (B, N) \n",
    "    encoder_mask = (encoder_mask.unsqueeze(-1)@encoder_mask.unsqueeze(1)).view(batch_size, 1, config[\"max_length\"], config[\"max_length\"]) # (B,1,N,N)\n",
    "    decoder_mask = torch.stack([(torch.tensor(testloader.dataset[\"attention_mask\"][i] != 0)).unsqueeze(0).int() & causal_mask(config[\"max_length\"]) for i in ix]) # (B,1,N,N)\n",
    "    return x, y, encoder_mask, decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, encoder_mask, decoder_mask = get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 128])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
