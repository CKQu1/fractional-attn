{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\diffuser\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\Anaconda3\\envs\\diffuser\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "import dgl\n",
    "import torch\n",
    "import dgl.function as fn\n",
    "\n",
    "dev = torch.device(f\"cuda:{torch.cuda.device_count()-1}\"\n",
    "                   if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)\n",
    "\n",
    "# dgl cuda\n",
    "src_ids = torch.tensor([2,3,4])\n",
    "dst_ids = torch.tensor([1,2,3])\n",
    "g_test = dgl.graph((src_ids, dst_ids))\n",
    "g_test = g_test.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\Kevin Qu\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.96it/s]\n",
      "Parameter 'function'=<function preprocess_function at 0x000001FD65649700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 25/25 [00:58<00:00,  2.34s/ba]\n",
      "100%|██████████| 25/25 [00:53<00:00,  2.13s/ba]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.07s/ba]\n",
      "100%|██████████| 25000/25000 [00:33<00:00, 744.35ex/s]\n",
      "100%|██████████| 25000/25000 [00:34<00:00, 734.67ex/s]\n",
      "100%|██████████| 50000/50000 [01:07<00:00, 741.21ex/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import TrainingArguments, DataCollatorWithPadding\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers.utils import logging\n",
    "from datasets import load_dataset,load_metric,load_from_disk\n",
    "from models.diffuser_utils import DiffuserConfig\n",
    "from graphtrainer import graphTrainer\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding = 'max_length', truncation=True, max_length = 1024)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1_score = metric_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"f1_score\": f1_score }\n",
    "\n",
    "metric_acc = load_metric('./metrics/accuracy')\n",
    "metric_f1 = load_metric('./metrics/f1')\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"./roberta-tokenizer\", max_length = 1024)\n",
    "tokenizer = RobertaTokenizer(tokenizer_file = \"./roberta-tokenizer/tokenizer.json\",\n",
    "                             vocab_file     = \"./roberta-tokenizer/vocab.json\",\n",
    "                             merges_file    = \"./roberta-tokenizer/merges.txt\",\n",
    "                             max_length     = 1024)\n",
    "\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "tokenized_imdb = tokenized_imdb.map(remove_columns=[\"text\"])\n",
    "# tokenized_imdb = load_from_disk(\"/home\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = DiffuserConfig.from_json_file(\"./models/config.json\")\n",
    "config = DiffuserConfig.from_json_file(\"./models/config_trial.json\")\n",
    "config.num_labels = 2\n",
    "with_frac = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffuserConfig {\n",
       "  \"attention_mode\": \"diffuser\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": [\n",
       "    64,\n",
       "    64,\n",
       "    64\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"ignore_attention_mask\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 4098,\n",
       "  \"model_type\": \"Diffuser\",\n",
       "  \"num_attention_heads\": 6,\n",
       "  \"num_glob\": 64,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"num_rand\": 64,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"sep_token_id\": 2,\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models.diffuser_utils.DiffuserConfig"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_setup = {\"with_frac\":True, \"gamma\":0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# sanity check 2\n",
    "from models.diffuser_att import DiffuserAttention\n",
    "attention_block = DiffuserAttention(config, **attn_setup)\n",
    "print(attention_block.with_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Kevin Qu\\AppData\\Local\\Temp\\ipykernel_14200\\1128065411.py\", line 3, in <module>\n",
      "    model =  DiffuserForSequenceClassification(config, **attn_setup)\n",
      "  File \"y:\\Diffuser\\models\\diffuser_app.py\", line 73, in __init__\n",
      "    self.classifier = DiffuserClassificationHead(config)\n",
      "  File \"y:\\Diffuser\\models\\diffuser.py\", line 54, in __init__\n",
      "    self.pooler = DiffuserPooler(config) if add_pooling_layer else None\n",
      "  File \"y:\\Diffuser\\models\\diffuser.py\", line 207, in __init__\n",
      "  File \"y:\\Diffuser\\models\\diffuser.py\", line 207, in <listcomp>\n",
      "  File \"y:\\Diffuser\\models\\diffuser.py\", line 305, in __init__\n",
      "    self.output = DiffuserOutput(config)\n",
      "  File \"y:\\Diffuser\\models\\diffuser_att.py\", line 28, in __init__\n",
      "    else:\n",
      "AttributeError: 'float' object has no attribute 'is_numeric'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diffuser\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from models.diffuser_app import DiffuserForSequenceClassification\n",
    "model =  DiffuserForSequenceClassification(config, **attn_setup)\n",
    "model = model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffuserLayer(\n",
       "  (attention): DiffuserAttention(\n",
       "    (self): DiffuserSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (output): DiffuserSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): DiffuserIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): DiffuserOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.diffuser.encoder.layer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m config\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      4\u001b[0m with_frac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m  \u001b[43mDiffuserForSequenceClassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_frac\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[0;32m      8\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      9\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./save_imdb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./save_imdb\",\n",
    "    learning_rate = 3e-5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    num_train_epochs = 1,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 2,\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    seed = 42,\n",
    "    warmup_steps = 200,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "if dev.type != \"cpu\":\n",
    "    steps_per_train_epoch       = int(len(tokenized_imdb['train'])/(training_args.per_device_train_batch_size*torch.cuda.device_count()*training_args.gradient_accumulation_steps ))\n",
    "else:\n",
    "    steps_per_train_epoch       = int(len(tokenized_imdb['train'])/(training_args.per_device_train_batch_size*torch.get_num_threads()*training_args.gradient_accumulation_steps ))\n",
    "training_args.eval_steps    = int(steps_per_train_epoch)\n",
    "training_args.logging_steps = int(steps_per_train_epoch/5)\n",
    "training_args.save_steps    = int(steps_per_train_epoch)\n",
    "\n",
    "trainer = graphTrainer(\n",
    "    model = model,\n",
    "    config = config,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_imdb[\"train\"],\n",
    "    eval_dataset = tokenized_imdb[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[0;32m      2\u001b[0m attention_window \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      3\u001b[0m     config\u001b[38;5;241m.\u001b[39mattention_window\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mattention_window, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(config\u001b[38;5;241m.\u001b[39mattention_window)\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m n_blocks \u001b[38;5;241m=\u001b[39m max_len\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m(attention_window\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m adj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([max_len, max_len])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "max_len = 4096\n",
    "attention_window = (\n",
    "    config.attention_window\n",
    "    if isinstance(config.attention_window, int)\n",
    "    else max(config.attention_window)\n",
    ")\n",
    "n_blocks = max_len//(attention_window//2)-1\n",
    "adj = np.zeros([max_len, max_len])\n",
    "# add local window att (overlap)\n",
    "for i in range(n_blocks):\n",
    "    start = i*attention_window//2\n",
    "    end = start+attention_window\n",
    "    if end > max_len:\n",
    "        end = max_len\n",
    "    adj[start:end, start:end] = 1\n",
    "\n",
    "# add random att\n",
    "np.random.seed(0)\n",
    "num_random = max_len*config.num_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(range(max_len*max_len), num_random ,replace=False)\n",
    "idx_x = idx %  max_len\n",
    "idx_y = idx // max_len\n",
    "adj[idx_x,idx_y] = 1\n",
    "\n",
    "# add global att\n",
    "num_global = config.num_glob\n",
    "idx = np.random.choice(range(attention_window,max_len), num_global ,replace=False)\n",
    "adj[idx,:] = 1\n",
    "adj[:,idx] = 1\n",
    "\n",
    "possible_seq_len = np.arange(attention_window, max_len+attention_window, attention_window)\n",
    "src_dst = {k: np.nonzero(adj[:k, :k]) for k in possible_seq_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_dst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m g_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B):\n\u001b[1;32m----> 5\u001b[0m     src,dst \u001b[38;5;241m=\u001b[39m \u001b[43msrc_dst\u001b[49m[seq_len]\n\u001b[0;32m      6\u001b[0m     g \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mgraph((src, dst))\n\u001b[0;32m      7\u001b[0m     g_list\u001b[38;5;241m.\u001b[39mappend(g)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'src_dst' is not defined"
     ]
    }
   ],
   "source": [
    "B, N, H, D = 5, 64, 3, 2\n",
    "batch_size, seq_len = B, N\n",
    "g_list = []\n",
    "for i in range(B):\n",
    "    src,dst = src_dst[seq_len]\n",
    "    g = dgl.graph((src, dst))\n",
    "    g_list.append(g)\n",
    "batched_g = dgl.batch(g_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvecs = torch.ones([B, N, H, D])\n",
    "kvecs = torch.ones([B, N, H, D])\n",
    "vvecs = torch.ones([B, N, H, D])\n",
    "batched_g.ndata['q'] = qvecs.reshape(-1, H, D)\n",
    "batched_g.ndata['k'] = kvecs.reshape(-1, H, D)\n",
    "batched_g.ndata['v'] = vvecs.reshape(-1, H, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_g.ndata['q'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import *\n",
    "batched_g.apply_edges(fn.u_dot_v('k', 'q', 'score'))   #score: [E,H,1]\n",
    "batched_g.apply_edges(mask_attention_score)   #kq\n",
    "e = batched_g.edata.pop('score')\n",
    "batched_g.edata['score'] = edge_softmax(batched_g, e)\n",
    "batched_g.edata['score']= nn.functional.dropout(batched_g.edata['score'],\n",
    "                                                p=config.attention_probs_dropout_prob,\n",
    "                                                training=False)\n",
    "\n",
    "batched_g.ndata[\"h\"] = batched_g.ndata[\"v\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('diffuser')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e4695c66188fd02ec302552241210d035f117e2ec380286f862195f0d03b1d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
